{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import os\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('dark')\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "import xgboost as xgb\n",
    "import xgbfir\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "pd.set_option('max_columns', None)\n",
    "\n",
    "SEED = 2131\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%run ../src/data/HotstarDataset.py\n",
    "%run ../src/features/util.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reload_data():\n",
    "    dataset = Hotstar('../data/raw/5f828822-4--4-hotstar_dataset/')\n",
    "    dataset.load_data('../data/processed/hotstar_processed.feather')\n",
    "    \n",
    "    data_processed = dataset.data\n",
    "    train_mask     = dataset.get_train_mask()\n",
    "    \n",
    "    return data_processed, train_mask\n",
    "\n",
    "def summation(viewership):\n",
    "    viewership = np.array(viewership)\n",
    "    viewership  = viewership.astype(np.int)\n",
    "    \n",
    "    return viewership.sum()\n",
    "    \n",
    "\n",
    "def data_preparation(data_processed, **params):\n",
    "    \"\"\"\n",
    "    Function to prepare dataset for modelling\n",
    "    \"\"\"\n",
    "    \n",
    "    st = time.time() # start time\n",
    "    \n",
    "    # viewership\n",
    "    viewership     = data_processed.cities.str.replace(r'[^\\d|^,]+', '').str.split(',').map(summation)\n",
    "    data_processed = data_processed.assign(viewership=viewership)\n",
    "    \n",
    "    print('Prepared viewership')\n",
    "    \n",
    "    del viewership\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    # number of cities involved\n",
    "    num_cities     = data_processed.cities.str.split(',').map(len)\n",
    "    data_processed = data_processed.assign(num_cities=num_cities)\n",
    "    \n",
    "    print('Prepared num cities')\n",
    "    del num_cities\n",
    "    gc.collect()\n",
    "    \n",
    "    # num genres watched\n",
    "    num_genres     = data_processed.genres.str.split(',').map(len)\n",
    "    data_processed = data_processed.assign(num_genres=num_genres)\n",
    "    \n",
    "    print('Prepared num genres')\n",
    "    del num_genres\n",
    "    gc.collect()\n",
    "    \n",
    "    # num titles watched\n",
    "    num_titles     = data_processed.titles.str.split(',').map(len)\n",
    "    data_processed = data_processed.assign(num_titles=num_titles)\n",
    "    \n",
    "    print('Prepared num titles')\n",
    "    del num_titles\n",
    "    gc.collect()\n",
    "    \n",
    "    # num tod\n",
    "    num_tod        = data_processed.tod.str.split(',').map(len)\n",
    "    data_processed = data_processed.assign(num_tod=num_tod)\n",
    "    \n",
    "    print('Prepared num tod')\n",
    "    del num_tod\n",
    "    gc.collect()\n",
    "    \n",
    "    # num DOW\n",
    "    num_dow        = data_processed.dow.str.split(',').map(len)\n",
    "    data_processed = data_processed.assign(num_dow=num_dow)\n",
    "    \n",
    "    print('Prepared num dow')\n",
    "    del num_dow\n",
    "    gc.collect()\n",
    "    \n",
    "    # dow OHE\n",
    "    dow_dict_train = data_processed.loc[train_mask, 'dow'].map(lambda x: x.split(','))\\\n",
    "                     .map(lambda x: dict((k.strip(), int(v.strip())) for k,v in \n",
    "                                          (item.split(':') for item in x)))\n",
    "\n",
    "    dow_dict_test  = data_processed.loc[~train_mask, 'dow'].map(lambda x: x.split(','))\\\n",
    "                         .map(lambda x: dict((k.strip(), int(v.strip())) for k,v in \n",
    "                                              (item.split(':') for item in x)))\n",
    "\n",
    "    dv     = DictVectorizer(sparse=False)\n",
    "    X1     = dv.fit_transform(dow_dict_train)\n",
    "    Xtest1 = dv.transform(dow_dict_test)\n",
    "    \n",
    "    X1 = pd.DataFrame(np.vstack((X1, Xtest1)), columns=['dow' + c for c in dv.get_feature_names()])\n",
    "    data_processed = pd.concat((data_processed, X1), axis='columns')\n",
    "    print('Prepared DOW OHE')\n",
    "    \n",
    "    # genres OHE\n",
    "    genres_dict_train = data_processed.loc[train_mask, 'genres'].map(lambda x: x.split(','))\\\n",
    "                     .map(lambda x: dict((k.strip(), int(v.strip())) for k,v in \n",
    "                                          (item.split(':') for item in x)))\n",
    "\n",
    "    genres_dict_test  = data_processed.loc[~train_mask, 'genres'].map(lambda x: x.split(','))\\\n",
    "                         .map(lambda x: dict((k.strip(), int(v.strip())) for k,v in \n",
    "                                              (item.split(':') for item in x)))\n",
    "\n",
    "    dv     = DictVectorizer(sparse=False)\n",
    "    X1     = dv.fit_transform(genres_dict_train)\n",
    "    Xtest1 = dv.transform(genres_dict_test)\n",
    "\n",
    "    X1 = pd.DataFrame(np.vstack((X1, Xtest1)), columns=dv.get_feature_names())\n",
    "    data_processed = pd.concat((data_processed, X1), axis='columns')\n",
    "    print('Prepared genres OHE')\n",
    "    \n",
    "    # tod OHE\n",
    "    tod_dict_train = data_processed.loc[train_mask, 'tod'].map(lambda x: x.split(','))\\\n",
    "                     .map(lambda x: dict((k.strip(), int(v.strip())) for k,v in \n",
    "                                          (item.split(':') for item in x)))\n",
    "\n",
    "    tod_dict_test  = data_processed.loc[~train_mask, 'tod'].map(lambda x: x.split(','))\\\n",
    "                         .map(lambda x: dict((k.strip(), int(v.strip())) for k,v in \n",
    "                                              (item.split(':') for item in x)))\n",
    "\n",
    "    dv     = DictVectorizer(sparse=False)\n",
    "    X1     = dv.fit_transform(tod_dict_train)\n",
    "    Xtest1 = dv.transform(tod_dict_test)\n",
    "\n",
    "    X1 = pd.DataFrame(np.vstack((X1, Xtest1)), columns=['tod' + col for col in dv.get_feature_names()])\n",
    "    data_processed = pd.concat((data_processed, X1), axis='columns')\n",
    "    \n",
    "    print('Prepared tod OHE')\n",
    "    \n",
    "    # convert watch time for three of the genres to hour.\n",
    "    data_processed = data_processed.assign(cricket_view_hour=data_processed.Cricket / 3600)\n",
    "    data_processed = data_processed.assign(romance_view_hour=data_processed.Romance / 3600)\n",
    "    data_processed = data_processed.assign(ts_view_hour=data_processed.TalkShow / 3600)\n",
    "    \n",
    "    # mask for cricket view hour, romance view hour and talk show view hour\n",
    "    data_processed = data_processed.assign(low_cricket_view=(data_processed.cricket_view_hour < data_processed.cricket_view_hour.quantile(q=.45)).astype('uint8'))\n",
    "    data_processed = data_processed.assign(high_romance_view=(data_processed.romance_view_hour > data_processed.romance_view_hour.quantile(q=.99)).astype('uint8'))\n",
    "    data_processed = data_processed.assign(high_ts_view=(data_processed.ts_view_hour > data_processed.ts_view_hour.quantile(q=.99)).astype('uint8'))\n",
    "    \n",
    "    if params['transform']:\n",
    "        # TFIDF transformer\n",
    "        vec = TfidfTransformer()\n",
    "        features_to_transform = ['dow1', 'dow2', 'dow3', 'dow4', 'dow5', 'dow6', 'dow7',\n",
    "           'Action', 'Athletics', 'Awards', 'Badminton', 'Boxing', 'Comedy',\n",
    "           'Cricket', 'Crime', 'Documentary', 'Drama', 'Family', 'Football',\n",
    "           'Formula1', 'FormulaE', 'Hockey', 'Horror', 'IndiaVsSa', 'Kabaddi',\n",
    "           'Kids', 'LiveTV', 'Mythology', 'NA', 'Reality', 'Romance', 'Science',\n",
    "           'Sport', 'Swimming', 'Table Tennis', 'TalkShow', 'Teen', 'Tennis',\n",
    "           'Thriller', 'Travel', 'Volleyball', 'Wildlife', 'tod0', 'tod1', 'tod10',\n",
    "           'tod11', 'tod12', 'tod13', 'tod14', 'tod15', 'tod16', 'tod17', 'tod18',\n",
    "           'tod19', 'tod2', 'tod20', 'tod21', 'tod22', 'tod23', 'tod3', 'tod4',\n",
    "           'tod5', 'tod6', 'tod7', 'tod8', 'tod9']\n",
    "\n",
    "        transformed = vec.fit_transform(data_processed.loc[:, features_to_transform])\n",
    "        transformed = pd.DataFrame(transformed.toarray(), columns=features_to_transform)\n",
    "\n",
    "        data_processed.drop(features_to_transform, axis='columns', inplace=True)\n",
    "        data_processed = pd.concat((data_processed, transformed), axis='columns')\n",
    "\n",
    "        print('TFIDF transformed')\n",
    "    \n",
    "    print('Prepared flags for cricket, romance and talkshow genres')\n",
    "    \n",
    "    # feature interaction between features\n",
    "    data_processed = data_processed.assign(cric_rom=data_processed.cricket_view_hour * data_processed.romance_view_hour)\n",
    "    data_processed = data_processed.assign(cric_ts=data_processed.cricket_view_hour * data_processed.ts_view_hour)\n",
    "    data_processed = data_processed.assign(rom_ts=data_processed.romance_view_hour * data_processed.ts_view_hour)\n",
    "    \n",
    "    print('Prepared Feature Interaction')\n",
    "    \n",
    "    # proportion of cricket watch time out of total viewership\n",
    "    proportion_cric_wt = (data_processed.Cricket) / (data_processed.viewership)\n",
    "    data_processed = data_processed.assign(proportion_cric_wt=proportion_cric_wt)\n",
    "    \n",
    "    print('Prepared cricket watch time proportion')\n",
    "    \n",
    "    # proportion of romance watch time out of total\n",
    "    prop_romance_wt = (data_processed.Romance) / (data_processed.viewership)\n",
    "    data_processed  = data_processed.assign(prop_romance_wt=prop_romance_wt)\n",
    "    \n",
    "    print('Prepared romance watch time proportion')\n",
    "    \n",
    "    # proportion of family watch time out of total\n",
    "    prop_family_wt = (data_processed.Family) / (data_processed.viewership)\n",
    "    data_processed  = data_processed.assign(prop_family_wt=prop_family_wt)\n",
    "    \n",
    "    print('Prepared family watch time proportion')\n",
    "    \n",
    "    # flag for those instances with very high viewership\n",
    "    data_processed = data_processed.assign(high_viewership=(data_processed.viewership > data_processed.viewership.quantile(q=.99)).astype('uint8'))\n",
    "    \n",
    "    print('Prepared high viewership')\n",
    "    \n",
    "    et = time.time() # end time\n",
    "    \n",
    "    print('It took: {} seconds to prepare data'.format(et - st))\n",
    "    \n",
    "    return data_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Dataset - 1 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared viewership\n",
      "Prepared num cities\n",
      "Prepared num genres\n",
      "Prepared num titles\n",
      "Prepared num tod\n",
      "Prepared num dow\n",
      "Prepared DOW OHE\n",
      "Prepared genres OHE\n",
      "Prepared tod OHE\n",
      "Prepared flags for cricket, romance and talkshow genres\n",
      "Prepared Feature Interaction\n",
      "Prepared cricket watch time proportion\n",
      "Prepared romance watch time proportion\n",
      "Prepared family watch time proportion\n",
      "Prepared high viewership\n",
      "It took: 23.61421036720276 seconds to prepare data\n"
     ]
    }
   ],
   "source": [
    "dataset1, train_mask = reload_data()\n",
    "\n",
    "params = {\n",
    "    'transform': False\n",
    "}\n",
    "\n",
    "dataset1 = data_preparation(dataset1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_file(dataset1, '../data/processed/hotstar_processed_experiment_20.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Dataset - 2 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.5516326427459717 seconds\n",
      "Took: 0.8438479900360107 seconds\n",
      "Took: 7.986990690231323 seconds\n",
      "Took: 1.664804220199585 seconds\n",
      "Took: 1.2268295288085938 seconds \n",
      "Took: 15.788763046264648 seconds\n"
     ]
    }
   ],
   "source": [
    "def prepare_titles(df):\n",
    "    def cleanup(titles):\n",
    "        cleaned_titles = []\n",
    "        \n",
    "        for title in titles:\n",
    "            cleaned_titles.append(re.sub(r'[,\"\\']', '', title))\n",
    "            \n",
    "        return ' '.join(cleaned_titles)\n",
    "    \n",
    "    return df.titles.str.split(r':\\d+').map(cleanup)\n",
    "\n",
    "def prepare_cities(df):\n",
    "    return df.cities.str.replace(r':\\d+', '').str.replace(',', ' ')\n",
    "\n",
    "def prepare_genres(df):\n",
    "    return df.genres.str.replace(r':\\d+', '').str.replace(',', ' ')\n",
    "\n",
    "def prepare_tod(df):\n",
    "    return df.tod.str.replace(r':\\d+', '').str.replace(',', ' ')\\\n",
    "             .map(lambda x: ' '.join(['tod_' + z for z in x.split()]))\n",
    "\n",
    "def prepare_dow(df):\n",
    "    return df.dow.str.replace(r':\\d+', '').str.replace(',', ' ')\\\n",
    "             .map(lambda x: ' '.join(['dow_' + z for z in x.split()]))\n",
    "\n",
    "\n",
    "st = time.time()\n",
    "cities_cleaned = prepare_cities(dataset1)\n",
    "et = time.time()\n",
    "\n",
    "print('Took: {} seconds'.format((et - st)))\n",
    "\n",
    "st = time.time()\n",
    "genres_cleaned = prepare_genres(dataset1)\n",
    "et = time.time()\n",
    "\n",
    "print('Took: {} seconds'.format((et - st)))\n",
    "\n",
    "st = time.time()\n",
    "titles_cleaned = prepare_titles(dataset1)\n",
    "et = time.time()\n",
    "\n",
    "print('Took: {} seconds'.format((et - st)))\n",
    "\n",
    "st = time.time()\n",
    "tod_cleaned = prepare_tod(dataset1)\n",
    "et = time.time()\n",
    "\n",
    "print('Took: {} seconds'.format((et - st)))\n",
    "\n",
    "st = time.time()\n",
    "dow_cleaned = prepare_dow(dataset1)\n",
    "et = time.time()\n",
    "\n",
    "print('Took: {} seconds '.format((et - st)))\n",
    "\n",
    "\n",
    "dataset2 = pd.concat((titles_cleaned, \n",
    "                      cities_cleaned, \n",
    "                      genres_cleaned,\n",
    "                      dow_cleaned,\n",
    "                      tod_cleaned\n",
    "                     ), axis='columns')\n",
    "\n",
    "st = time.time()\n",
    "dataset2 = dataset2\\\n",
    "           .apply(lambda x: x['titles'] + ' ' + x['cities'] + ' ' + x['genres'] + ' ' + x['dow'] + ' ' + x['tod'], axis='columns')\n",
    "et = time.time()\n",
    "\n",
    "print('Took: {} seconds'.format((et - st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_file(dataset2.reset_index(), '../data/processed/hotstar_processed_2_experiment_20.feather')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Split into training and test dataset **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = [\n",
    "           'viewership', 'num_cities', 'num_genres', 'num_titles', 'num_tod',\n",
    "           'num_dow', 'low_cricket_view',\n",
    "            'dow1', 'dow2', 'dow3', 'dow4', 'dow5', 'dow6', 'dow7',\n",
    "            'Action', 'Athletics', 'Awards', 'Badminton', 'Boxing', 'Comedy',\n",
    "            'Cricket', 'Crime', 'Documentary', 'Drama', 'Family', 'Football',\n",
    "            'Formula1', 'FormulaE', 'Hockey', 'Horror', 'IndiaVsSa', 'Kabaddi',\n",
    "            'Kids', 'LiveTV', 'Mythology', 'NA', 'Reality', 'Romance', 'Science',\n",
    "            'Sport', 'Swimming', 'Table Tennis', 'TalkShow', 'Teen', 'Tennis',\n",
    "            'Thriller', 'Travel', 'Volleyball', 'Wildlife',\n",
    "            'tod0', 'tod1', 'tod10',\n",
    "            'tod11', 'tod12', 'tod13', 'tod14', 'tod15', 'tod16', 'tod17', 'tod18',\n",
    "            'tod19', 'tod2', 'tod20', 'tod21', 'tod22', 'tod23', 'tod3', 'tod4',\n",
    "            'tod5', 'tod6', 'tod7', 'tod8', 'tod9',\n",
    "           'high_romance_view', 'high_ts_view', 'cric_rom', 'cric_ts', 'rom_ts',\n",
    "           'proportion_cric_wt'\n",
    "           ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took: 20.8608341217041 seconds to vectorize\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "vec = TfidfVectorizer(min_df=3)\n",
    "\n",
    "X2      = vec.fit_transform(dataset2.loc[train_mask])\n",
    "Xtest2  = vec.transform(dataset2.loc[~train_mask])\n",
    "\n",
    "et = time.time()\n",
    "print('It took: {} seconds to vectorize'.format((et - st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X1     = dataset1.loc[train_mask, features]\n",
    "Xtest1 = dataset1.loc[~train_mask, features]\n",
    "\n",
    "y = dataset1.loc[train_mask, 'segment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train, y_test = train_test_split(X1, y, stratify=y, test_size=.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train2 = X2[X_train1.index.values]\n",
    "X_test2  = X2[X_test1.index.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cross_validation_multiple_dataset(X1, X2, y):\n",
    "    skf = StratifiedKFold(n_splits=10, random_state=SEED)\n",
    "    \n",
    "    fold_scores = []\n",
    "    for itr, ite in tqdm_notebook(skf.split(X1, y)):\n",
    "        Xtr1 = X1.iloc[itr]\n",
    "        Xtr2 = X2[itr]\n",
    "        \n",
    "        ytr  = y.iloc[itr]\n",
    "        \n",
    "        Xte1 = X1.iloc[ite]\n",
    "        Xte2 = X2[ite]\n",
    "        \n",
    "        yte = y.iloc[ite]\n",
    "        \n",
    "        model1 = xgb.XGBClassifier(n_estimators=200, max_depth=4, colsample_bytree=1.)\n",
    "        model2 = LogisticRegression(C=1., random_state=SEED)\n",
    "        \n",
    "        model1.fit(Xtr1, ytr)\n",
    "        model2.fit(Xtr2, ytr)\n",
    "        \n",
    "        xgb_preds = model1.predict_proba(Xte1)[:, 1]\n",
    "        log_preds = model2.predict_proba(Xte2)[:, 1]\n",
    "        \n",
    "        print('XGB AUC: {}'.format(roc_auc_score(yte, xgb_preds)))\n",
    "        print('Log AUC: {}'.format(roc_auc_score(yte, log_preds)))\n",
    "        \n",
    "        xgb_rank  = sp.stats.rankdata(xgb_preds)\n",
    "        log_rank  = sp.stats.rankdata(log_preds)\n",
    "        \n",
    "        ensemble_ranks = xgb_preds * .4 + .6 * log_rank\n",
    "        fold_auc = roc_auc_score(yte, ensemble_ranks)\n",
    "        \n",
    "        print('Ensemble AUC: {}'.format(fold_auc))\n",
    "        \n",
    "        fold_scores.append(fold_auc)\n",
    "        print('='*75)\n",
    "        \n",
    "    print('Mean AUC: {0} and std: {1}'.format(np.mean(fold_scores), np.std(fold_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB AUC: 0.8062499792202091\n",
      "Log AUC: 0.8200466243091413\n",
      "Ensemble AUC: 0.8200466797219166\n",
      "===========================================================================\n",
      "XGB AUC: 0.8144378540183681\n",
      "Log AUC: 0.8308934534239\n",
      "Ensemble AUC: 0.8308934811302876\n",
      "===========================================================================\n",
      "XGB AUC: 0.8128042576960032\n",
      "Log AUC: 0.8259704716402957\n",
      "Ensemble AUC: 0.8259705270530711\n",
      "===========================================================================\n",
      "XGB AUC: 0.8124701463673047\n",
      "Log AUC: 0.8215171685401726\n",
      "Ensemble AUC: 0.8215171962465602\n",
      "===========================================================================\n",
      "XGB AUC: 0.8042658444064864\n",
      "Log AUC: 0.8211801781317244\n",
      "Ensemble AUC: 0.8211801504026265\n",
      "===========================================================================\n",
      "XGB AUC: 0.8236700015528295\n",
      "Log AUC: 0.8384297289203398\n",
      "Ensemble AUC: 0.8384297289203398\n",
      "===========================================================================\n",
      "XGB AUC: 0.8118836929670921\n",
      "Log AUC: 0.823369501918429\n",
      "Ensemble AUC: 0.823369501918429\n",
      "===========================================================================\n",
      "XGB AUC: 0.8228634116419067\n",
      "Log AUC: 0.834145065162243\n",
      "Ensemble AUC: 0.8341450374312688\n",
      "===========================================================================\n",
      "XGB AUC: 0.8216535647057911\n",
      "Log AUC: 0.834915071118856\n",
      "Ensemble AUC: 0.8349150156569078\n",
      "===========================================================================\n",
      "XGB AUC: 0.8037291504671561\n",
      "Log AUC: 0.8201252275326424\n",
      "Ensemble AUC: 0.8201253107255645\n",
      "===========================================================================\n",
      "\n",
      "Mean AUC: 0.8270592629206973 and std: 0.006582126882878879\n"
     ]
    }
   ],
   "source": [
    "cross_validation_multiple_dataset(X_train1, X_train2, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# full training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "02753f1851e14c7d81acb01c8bddc69f": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
