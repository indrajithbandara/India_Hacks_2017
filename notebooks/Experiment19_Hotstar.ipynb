{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abhishek/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import gc\n",
    "import time\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('dark')\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import xgboost as xgb\n",
    "import xgbfir\n",
    "\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "pd.set_option('max_columns', None)\n",
    "\n",
    "SEED = 2131\n",
    "np.random.seed(SEED)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%run ../src/data/HotstarDataset.py\n",
    "%run ../src/features/categorical_features.py\n",
    "%run ../src/features/util.py\n",
    "%run ../src/models/cross_validation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reload_data():\n",
    "    dataset = Hotstar('../data/raw/5f828822-4--4-hotstar_dataset/')\n",
    "    dataset.load_data('../data/processed/hotstar_processed.feather')\n",
    "    \n",
    "    data_processed = dataset.data\n",
    "    train_mask     = dataset.get_train_mask()\n",
    "    \n",
    "    return data_processed, train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summation(viewership):\n",
    "    viewership = np.array(viewership)\n",
    "    viewership  = viewership.astype(np.int)\n",
    "    \n",
    "    return viewership.sum()\n",
    "    \n",
    "\n",
    "def data_preparation(data_processed, **params):\n",
    "    \"\"\"\n",
    "    Function to prepare dataset for modelling\n",
    "    \"\"\"\n",
    "    \n",
    "    st = time.time() # start time\n",
    "    \n",
    "    # viewership\n",
    "    viewership     = data_processed.cities.str.replace(r'[^\\d|^,]+', '').str.split(',').map(summation)\n",
    "    data_processed = data_processed.assign(viewership=viewership)\n",
    "    \n",
    "    print('Prepared viewership')\n",
    "    \n",
    "    del viewership\n",
    "    gc.collect()\n",
    "    \n",
    "    \n",
    "    # number of cities involved\n",
    "    num_cities     = data_processed.cities.str.split(',').map(len)\n",
    "    data_processed = data_processed.assign(num_cities=num_cities)\n",
    "    \n",
    "    print('Prepared num cities')\n",
    "    del num_cities\n",
    "    gc.collect()\n",
    "    \n",
    "    # num genres watched\n",
    "    num_genres     = data_processed.genres.str.split(',').map(len)\n",
    "    data_processed = data_processed.assign(num_genres=num_genres)\n",
    "    \n",
    "    print('Prepared num genres')\n",
    "    del num_genres\n",
    "    gc.collect()\n",
    "    \n",
    "    # num titles watched\n",
    "    num_titles     = data_processed.titles.str.split(',').map(len)\n",
    "    data_processed = data_processed.assign(num_titles=num_titles)\n",
    "    \n",
    "    print('Prepared num titles')\n",
    "    del num_titles\n",
    "    gc.collect()\n",
    "    \n",
    "    # num tod\n",
    "    num_tod        = data_processed.tod.str.split(',').map(len)\n",
    "    data_processed = data_processed.assign(num_tod=num_tod)\n",
    "    \n",
    "    print('Prepared num tod')\n",
    "    del num_tod\n",
    "    gc.collect()\n",
    "    \n",
    "    # num DOW\n",
    "    num_dow        = data_processed.dow.str.split(',').map(len)\n",
    "    data_processed = data_processed.assign(num_dow=num_dow)\n",
    "    \n",
    "    print('Prepared num dow')\n",
    "    del num_dow\n",
    "    gc.collect()\n",
    "    \n",
    "    # dow OHE\n",
    "    dow_dict_train = data_processed.loc[train_mask, 'dow'].map(lambda x: x.split(','))\\\n",
    "                     .map(lambda x: dict((k.strip(), int(v.strip())) for k,v in \n",
    "                                          (item.split(':') for item in x)))\n",
    "\n",
    "    dow_dict_test  = data_processed.loc[~train_mask, 'dow'].map(lambda x: x.split(','))\\\n",
    "                         .map(lambda x: dict((k.strip(), int(v.strip())) for k,v in \n",
    "                                              (item.split(':') for item in x)))\n",
    "\n",
    "    dv     = DictVectorizer(sparse=False)\n",
    "    X1     = dv.fit_transform(dow_dict_train)\n",
    "    Xtest1 = dv.transform(dow_dict_test)\n",
    "    \n",
    "    X1 = pd.DataFrame(np.vstack((X1, Xtest1)), columns=['dow' + c for c in dv.get_feature_names()])\n",
    "    data_processed = pd.concat((data_processed, X1), axis='columns')\n",
    "    print('Prepared DOW OHE')\n",
    "    \n",
    "    # genres OHE\n",
    "    genres_dict_train = data_processed.loc[train_mask, 'genres'].map(lambda x: x.split(','))\\\n",
    "                     .map(lambda x: dict((k.strip(), int(v.strip())) for k,v in \n",
    "                                          (item.split(':') for item in x)))\n",
    "\n",
    "    genres_dict_test  = data_processed.loc[~train_mask, 'genres'].map(lambda x: x.split(','))\\\n",
    "                         .map(lambda x: dict((k.strip(), int(v.strip())) for k,v in \n",
    "                                              (item.split(':') for item in x)))\n",
    "\n",
    "    dv     = DictVectorizer(sparse=False)\n",
    "    X1     = dv.fit_transform(genres_dict_train)\n",
    "    Xtest1 = dv.transform(genres_dict_test)\n",
    "\n",
    "    X1 = pd.DataFrame(np.vstack((X1, Xtest1)), columns=dv.get_feature_names())\n",
    "    data_processed = pd.concat((data_processed, X1), axis='columns')\n",
    "    print('Prepared genres OHE')\n",
    "    \n",
    "    # tod OHE\n",
    "    tod_dict_train = data_processed.loc[train_mask, 'tod'].map(lambda x: x.split(','))\\\n",
    "                     .map(lambda x: dict((k.strip(), int(v.strip())) for k,v in \n",
    "                                          (item.split(':') for item in x)))\n",
    "\n",
    "    tod_dict_test  = data_processed.loc[~train_mask, 'tod'].map(lambda x: x.split(','))\\\n",
    "                         .map(lambda x: dict((k.strip(), int(v.strip())) for k,v in \n",
    "                                              (item.split(':') for item in x)))\n",
    "\n",
    "    dv     = DictVectorizer(sparse=False)\n",
    "    X1     = dv.fit_transform(tod_dict_train)\n",
    "    Xtest1 = dv.transform(tod_dict_test)\n",
    "\n",
    "    X1 = pd.DataFrame(np.vstack((X1, Xtest1)), columns=['tod' + col for col in dv.get_feature_names()])\n",
    "    data_processed = pd.concat((data_processed, X1), axis='columns')\n",
    "    \n",
    "    print('Prepared tod OHE')\n",
    "    \n",
    "    # convert watch time for three of the genres to hour.\n",
    "    data_processed = data_processed.assign(cricket_view_hour=data_processed.Cricket / 3600)\n",
    "    data_processed = data_processed.assign(romance_view_hour=data_processed.Romance / 3600)\n",
    "    data_processed = data_processed.assign(ts_view_hour=data_processed.TalkShow / 3600)\n",
    "    \n",
    "    # mask for cricket view hour, romance view hour and talk show view hour\n",
    "    data_processed = data_processed.assign(low_cricket_view=(data_processed.cricket_view_hour < data_processed.cricket_view_hour.quantile(q=.45)).astype('uint8'))\n",
    "    data_processed = data_processed.assign(high_romance_view=(data_processed.romance_view_hour > data_processed.romance_view_hour.quantile(q=.99)).astype('uint8'))\n",
    "    data_processed = data_processed.assign(high_ts_view=(data_processed.ts_view_hour > data_processed.ts_view_hour.quantile(q=.99)).astype('uint8'))\n",
    "    \n",
    "    if params['transform']:\n",
    "        # TFIDF transformer\n",
    "        vec = TfidfTransformer()\n",
    "        features_to_transform = ['dow1', 'dow2', 'dow3', 'dow4', 'dow5', 'dow6', 'dow7',\n",
    "           'Action', 'Athletics', 'Awards', 'Badminton', 'Boxing', 'Comedy',\n",
    "           'Cricket', 'Crime', 'Documentary', 'Drama', 'Family', 'Football',\n",
    "           'Formula1', 'FormulaE', 'Hockey', 'Horror', 'IndiaVsSa', 'Kabaddi',\n",
    "           'Kids', 'LiveTV', 'Mythology', 'NA', 'Reality', 'Romance', 'Science',\n",
    "           'Sport', 'Swimming', 'Table Tennis', 'TalkShow', 'Teen', 'Tennis',\n",
    "           'Thriller', 'Travel', 'Volleyball', 'Wildlife', 'tod0', 'tod1', 'tod10',\n",
    "           'tod11', 'tod12', 'tod13', 'tod14', 'tod15', 'tod16', 'tod17', 'tod18',\n",
    "           'tod19', 'tod2', 'tod20', 'tod21', 'tod22', 'tod23', 'tod3', 'tod4',\n",
    "           'tod5', 'tod6', 'tod7', 'tod8', 'tod9']\n",
    "\n",
    "        transformed = vec.fit_transform(data_processed.loc[:, features_to_transform])\n",
    "        transformed = pd.DataFrame(transformed.toarray(), columns=features_to_transform)\n",
    "\n",
    "        data_processed.drop(features_to_transform, axis='columns', inplace=True)\n",
    "        data_processed = pd.concat((data_processed, transformed), axis='columns')\n",
    "\n",
    "        print('TFIDF transformed')\n",
    "    \n",
    "    print('Prepared flags for cricket, romance and talkshow genres')\n",
    "    \n",
    "    # feature interaction between features\n",
    "    data_processed = data_processed.assign(cric_rom=data_processed.cricket_view_hour * data_processed.romance_view_hour)\n",
    "    data_processed = data_processed.assign(cric_ts=data_processed.cricket_view_hour * data_processed.ts_view_hour)\n",
    "    data_processed = data_processed.assign(rom_ts=data_processed.romance_view_hour * data_processed.ts_view_hour)\n",
    "    \n",
    "    print('Prepared Feature Interaction')\n",
    "    \n",
    "    # proportion of cricket watch time out of total viewership\n",
    "    proportion_cric_wt = (data_processed.Cricket) / (data_processed.viewership)\n",
    "    data_processed = data_processed.assign(proportion_cric_wt=proportion_cric_wt)\n",
    "    \n",
    "    print('Prepared cricket watch time proportion')\n",
    "    \n",
    "    # proportion of romance watch time out of total\n",
    "    prop_romance_wt = (data_processed.Romance) / (data_processed.viewership)\n",
    "    data_processed  = data_processed.assign(prop_romance_wt=prop_romance_wt)\n",
    "    \n",
    "    print('Prepared romance watch time proportion')\n",
    "    \n",
    "    # proportion of family watch time out of total\n",
    "    prop_family_wt = (data_processed.Family) / (data_processed.viewership)\n",
    "    data_processed  = data_processed.assign(prop_family_wt=prop_family_wt)\n",
    "    \n",
    "    print('Prepared family watch time proportion')\n",
    "    \n",
    "    # flag for those instances with very high viewership\n",
    "    data_processed = data_processed.assign(high_viewership=(data_processed.viewership > data_processed.viewership.quantile(q=.99)).astype('uint8'))\n",
    "    \n",
    "    print('Prepared high viewership')\n",
    "    \n",
    "    et = time.time() # end time\n",
    "    \n",
    "    print('It took: {} seconds to prepare data'.format(et - st))\n",
    "    \n",
    "    return data_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared viewership\n",
      "Prepared num cities\n",
      "Prepared num genres\n",
      "Prepared num titles\n",
      "Prepared num tod\n",
      "Prepared num dow\n",
      "Prepared DOW OHE\n",
      "Prepared genres OHE\n",
      "Prepared tod OHE\n",
      "Prepared flags for cricket, romance and talkshow genres\n",
      "Prepared Feature Interaction\n",
      "Prepared cricket watch time proportion\n",
      "Prepared romance watch time proportion\n",
      "Prepared family watch time proportion\n",
      "Prepared high viewership\n",
      "It took: 21.772369384765625 seconds to prepare data\n"
     ]
    }
   ],
   "source": [
    "data_processed, train_mask = reload_data()\n",
    "\n",
    "params = {\n",
    "    'transform': False\n",
    "}\n",
    "\n",
    "data_processed = data_preparation(data_processed, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_titles(df):\n",
    "    def cleanup(titles):\n",
    "        cleaned_titles = []\n",
    "        \n",
    "        for title in titles:\n",
    "            title_replaced = re.sub(r'[,\"\\']', '', title)\n",
    "            cleaned_titles.append(title_replaced)\n",
    "            \n",
    "        return ' '.join(cleaned_titles)\n",
    "    \n",
    "    return df.titles.str.split(r':\\d+').map(cleanup)\n",
    "\n",
    "def prepare_cities(df):\n",
    "    return df.cities.str.replace(r':\\d+', '').str.replace(',', ' ')\n",
    "\n",
    "def prepare_genres(df):\n",
    "    return df.genres.str.replace(r':\\d+', '').str.replace(',', ' ')\n",
    "\n",
    "def prepare_tod(df):\n",
    "    return df.tod.str.replace(r':\\d+', '').str.replace(',', ' ')\n",
    "\n",
    "def prepare_dow(df):\n",
    "    return df.dow.str.replace(r':\\d+', '').str.replace(',', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.5889754295349121 seconds\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "cities_cleaned = prepare_cities(data_processed)\n",
    "et = time.time()\n",
    "\n",
    "print('Took: {} seconds'.format((et - st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.7167510986328125 seconds\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "genres_cleaned = prepare_genres(data_processed)\n",
    "et = time.time()\n",
    "\n",
    "print('Took: {} seconds'.format((et - st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 7.565122365951538 seconds\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "titles_cleaned = prepare_titles(data_processed)\n",
    "et = time.time()\n",
    "\n",
    "print('Took: {} seconds'.format((et - st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 0.7164156436920166 seconds \n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "dow_cleaned = prepare_dow(data_processed)\n",
    "et = time.time()\n",
    "\n",
    "print('Took: {} seconds '.format((et - st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 1.0518476963043213 seconds\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "tod_cleaned = prepare_tod(data_processed)\n",
    "et = time.time()\n",
    "\n",
    "print('Took: {} seconds'.format((et - st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text_data = pd.concat((titles_cleaned, cities_cleaned, \n",
    "                       genres_cleaned, dow_cleaned,\n",
    "                       tod_cleaned\n",
    "                      ), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 17.23492455482483 seconds\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "text_data = text_data.apply(lambda x: x['titles'] + ' ' + \\\n",
    "                                      x['cities'] + ' ' + \\\n",
    "                                      x['genres'] + ' ' + \\\n",
    "                                      x['dow'] + ' ' + \\\n",
    "                                      x['tod'], axis='columns')\n",
    "et = time.time()\n",
    "\n",
    "print('Took: {} seconds'.format((et - st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 19.110977172851562 seconds\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "vec = TfidfVectorizer(min_df=3)\n",
    "\n",
    "train_transformed = vec.fit_transform(text_data.loc[train_mask])\n",
    "test_transformed  = vec.transform(text_data.loc[~train_mask])\n",
    "\n",
    "et = time.time()\n",
    "\n",
    "print('Took: {} seconds'.format((et - st)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = data_processed.loc[train_mask, 'segment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    'stratify': y,\n",
    "    'test_size': .2,\n",
    "    'random_state': SEED\n",
    "}\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_split(train_transformed, y, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validate_single_model_sign(X, y, model, ret_fold_preds=False,\n",
    "                   save_folds=False, plot_cv_scores=False):\n",
    "    \"\"\"\n",
    "    Stratified K-Fold with 10 splits and then save each fold\n",
    "    and analyze the performance of the model on each fold\n",
    "    \"\"\"\n",
    "    \n",
    "    skf = StratifiedKFold(n_splits=10, random_state=SEED)\n",
    "    fold_counter = 0\n",
    "    \n",
    "    cv_scores = []\n",
    "    preds     = []\n",
    "    \n",
    "    for (itr, ite) in tqdm_notebook(skf.split(X, y)):\n",
    "        Xtr = X[itr]\n",
    "        ytr = y.iloc[itr]\n",
    "        \n",
    "        Xte = X[ite]\n",
    "        yte = y.iloc[ite]\n",
    "        \n",
    "        if save_folds:\n",
    "            save_file(pd.concat((Xtr, ytr), axis='columns'), '../data/processed/train_fold%s.feather'%(fold_counter))\n",
    "            save_file(pd.concat((Xte, yte), axis='columns'), '../data/processed/test_fold%s.feather'%(fold_counter))\n",
    "        \n",
    "        print('Training model')\n",
    "        start_time = time.time()\n",
    "        model.fit(Xtr, ytr)\n",
    "        end_time   = time.time()\n",
    "        \n",
    "        print('Took: {} seconds to train model'.format(end_time - start_time))\n",
    "        \n",
    "        start_time  = time.time()\n",
    "        fold_preds  = model.predict_proba(Xte)[:, 1]\n",
    "        end_time    = time.time()\n",
    "        \n",
    "        if ret_fold_preds:\n",
    "            preds.append(fold_preds)\n",
    "        \n",
    "        print('Took: {} seconds to generate predictions'.format(end_time - start_time))\n",
    "        \n",
    "        fold_score = roc_auc_score(yte, fold_preds)\n",
    "        print('Fold log loss score: {}'.format(fold_score))\n",
    "        \n",
    "        cv_scores.append(fold_score)\n",
    "        print('='*75)\n",
    "        print('\\n')\n",
    "        \n",
    "    if plot_cv_scores:\n",
    "        plt.scatter(np.arange(0, len(cv_scores)), cv_scores)\n",
    "    \n",
    "    print('Mean cv score: {} \\n Std cv score: {}'.format(np.mean(cv_scores), np.std(cv_scores)))\n",
    "    \n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression(C=.1, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model\n",
      "Took: 1.4320430755615234 seconds to train model\n",
      "Took: 0.0031549930572509766 seconds to generate predictions\n",
      "Fold log loss score: 0.8185499529545539\n",
      "===========================================================================\n",
      "\n",
      "\n",
      "Training model\n",
      "Took: 1.3673627376556396 seconds to train model\n",
      "Took: 0.002483844757080078 seconds to generate predictions\n",
      "Fold log loss score: 0.8289553916076243\n",
      "===========================================================================\n",
      "\n",
      "\n",
      "Training model\n",
      "Took: 1.3045082092285156 seconds to train model\n",
      "Took: 0.0024766921997070312 seconds to generate predictions\n",
      "Fold log loss score: 0.825217827619722\n",
      "===========================================================================\n",
      "\n",
      "\n",
      "Training model\n",
      "Took: 1.4631121158599854 seconds to train model\n",
      "Took: 0.003160238265991211 seconds to generate predictions\n",
      "Fold log loss score: 0.8205757054877487\n",
      "===========================================================================\n",
      "\n",
      "\n",
      "Training model\n",
      "Took: 1.5363333225250244 seconds to train model\n",
      "Took: 0.0024902820587158203 seconds to generate predictions\n",
      "Fold log loss score: 0.8185754453293107\n",
      "===========================================================================\n",
      "\n",
      "\n",
      "Training model\n",
      "Took: 1.4928629398345947 seconds to train model\n",
      "Took: 0.0033767223358154297 seconds to generate predictions\n",
      "Fold log loss score: 0.8382807127487301\n",
      "===========================================================================\n",
      "\n",
      "\n",
      "Training model\n",
      "Took: 1.475733995437622 seconds to train model\n",
      "Took: 0.0027108192443847656 seconds to generate predictions\n",
      "Fold log loss score: 0.8209046897514084\n",
      "===========================================================================\n",
      "\n",
      "\n",
      "Training model\n",
      "Took: 1.4621376991271973 seconds to train model\n",
      "Took: 0.0026366710662841797 seconds to generate predictions\n",
      "Fold log loss score: 0.8333716205648466\n",
      "===========================================================================\n",
      "\n",
      "\n",
      "Training model\n",
      "Took: 1.466433048248291 seconds to train model\n",
      "Took: 0.002537965774536133 seconds to generate predictions\n",
      "Fold log loss score: 0.8322241960513311\n",
      "===========================================================================\n",
      "\n",
      "\n",
      "Training model\n",
      "Took: 1.4692034721374512 seconds to train model\n",
      "Took: 0.0024995803833007812 seconds to generate predictions\n",
      "Fold log loss score: 0.8177716720335346\n",
      "===========================================================================\n",
      "\n",
      "\n",
      "\n",
      "Mean cv score: 0.8254427214148811 \n",
      " Std cv score: 0.006955972990696794\n"
     ]
    }
   ],
   "source": [
    "params = {\n",
    "    'ret_fold_preds': True,\n",
    "    'save_folds': False,\n",
    "    'plot_cv_scores': False\n",
    "}\n",
    "\n",
    "cv_scores = cross_validate_single_model_sign(X_train, y_train, model, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC score: 0.8203640717702794\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train)\n",
    "\n",
    "preds = model.predict_proba(X_test)[:, 1]\n",
    "print('ROC AUC score: {}'.format(roc_auc_score(y_test, preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took: 5.538108587265015 seconds to train model\n"
     ]
    }
   ],
   "source": [
    "st = time.time()\n",
    "model.fit(train_transformed, y)\n",
    "et = time.time()\n",
    "\n",
    "preds = model.predict_proba(test_transformed)[:, 1]\n",
    "print('Took: {} seconds to train model'.format(et - st))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub            = pd.read_csv('../data/raw/5f828822-4--4-hotstar_dataset/sample_submission.csv')\n",
    "sub['segment'] = preds\n",
    "sub['ID']      = data_processed.loc[~train_mask, 'ID'].values\n",
    "sub.to_csv('../submissions/hotstar/logistic_reg_experiment_19_tod_dow.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "1a2507ce9969405dbb5fb7d1dd6ef5fa": {
     "views": [
      {
       "cell_index": 17
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
