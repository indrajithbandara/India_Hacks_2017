{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple examples of ensembling models\n",
    "\n",
    "Expected that hyperparameters of base models are already tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/swat/anaconda3/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SHIFT = 190\n",
    "os.environ['LIGHTGBM_EXEC'] = '/usr/local/bin/lightgbm'  # path to LightGBM executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 132/132 [00:30<00:00,  4.30it/s]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('~/data/allstate/train.csv.zip', compression='zip')\n",
    "\n",
    "for column in tqdm(df.columns):\n",
    "    encoder = LabelEncoder()\n",
    "    if column.startswith('cat') :\n",
    "        df[column] = encoder.fit_transform(df[column])\n",
    "        df['{}_sz'.format(column)] = df[column].map(df.groupby(column).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### For speed and simplicity there are only one split 80% / 20%\n",
    "This is equivalent to a single iteration of 5-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = 0.2, random_state = 0)\n",
    "y_train, y_test = train['loss'], test['loss']\n",
    "del train['loss'], test['loss']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fit simple XGBoost and LightGBM models (without any tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1162.57690445\n",
      "CPU times: user 4min 55s, sys: 1.35 s, total: 4min 56s\n",
      "Wall time: 1min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dtrain = xgb.DMatrix(train, label=np.log(y_train + SHIFT))\n",
    "dtest = xgb.DMatrix(test)\n",
    "params = {'silent': 1}\n",
    "model = xgb.train(params, dtrain, 100)\n",
    "y_pred_1 = np.exp(model.predict(dtest)) - SHIFT\n",
    "print(mean_absolute_error(y_pred_1, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1150.66772131\n",
      "CPU times: user 27 s, sys: 1.21 s, total: 28.2 s\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pylightgbm.models import GBMRegressor\n",
    "params = {'verbose': False, 'num_iterations': 100}\n",
    "clf = GBMRegressor(**params)\n",
    "clf.fit(train, np.log(y_train + SHIFT))\n",
    "y_pred_2 = np.exp(clf.predict(test)) - SHIFT\n",
    "print(mean_absolute_error(y_pred_2, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get mean of this predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1147.67752573\n"
     ]
    }
   ],
   "source": [
    "y_pred = (y_pred_1 + y_pred_2)/2\n",
    "print(mean_absolute_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Try to choose weights for convex combination of predictions\n",
    "i.e the sum of the weights is equal to one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 1149.49742146\n",
      "0.1 1148.50355639\n",
      "0.15 1147.7101578\n",
      "0.2 1147.12598108\n",
      "0.25 1146.74645865\n",
      "0.3 1146.56618344\n",
      "0.35 1146.57364663\n",
      "0.4 1146.76553844\n",
      "0.45 1147.12663828\n",
      "0.5 1147.67752573\n",
      "0.55 1148.38881233\n",
      "0.6 1149.24503975\n",
      "0.65 1150.28968449\n",
      "0.7 1151.50808495\n",
      "0.75 1152.90743673\n",
      "0.8 1154.48431484\n",
      "0.85 1156.22900358\n",
      "0.9 1158.16242752\n",
      "0.95 1160.28705474\n",
      "----------------\n",
      "best: 0.3 1146.56618344\n"
     ]
    }
   ],
   "source": [
    "best_a = None\n",
    "min_err = 10**6\n",
    "for a in np.arange(.05, 1, .05):\n",
    "    y_pred = a*y_pred_1 + (1 - a)*y_pred_2\n",
    "    err = mean_absolute_error(y_pred, y_test)\n",
    "    if err < min_err:\n",
    "        min_err = err\n",
    "        best_a = a\n",
    "    print(a, err)\n",
    "print('----------------')\n",
    "print('best:', best_a, min_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is better than simple mean.\n",
    "\n",
    "When using such method better models will have larger weight. This can be a problem if you have models with different quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get out of fold predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "from pylightgbm.models import GBMRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "N_SPLITS = 5\n",
    "\n",
    "def get_xgb_pred(X_train, X_test, y_train, params=None, n_trees=100, res_transform=None):\n",
    "    if params is None:\n",
    "        params = {'silent': 1}\n",
    "    if res_transform is None:\n",
    "        res_transform = lambda x: x\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    model = xgb.train(params, dtrain, n_trees)\n",
    "    return res_transform(model.predict(dtest))\n",
    "\n",
    "def get_lightgbm_pred(X_train, X_test, y_train, params=None, n_trees=100, res_transform=None):\n",
    "    if params is None:\n",
    "        params = {'verbose': False}\n",
    "    if not 'num_iterations' in params:\n",
    "        params['num_iterations'] = n_trees\n",
    "    if res_transform is None:\n",
    "        res_transform = lambda x: x\n",
    "    clf = GBMRegressor(**params)\n",
    "    clf.fit(X_train, y_train)\n",
    "    return res_transform(clf.predict(X_test))\n",
    "\n",
    "def predict_outoffolds(X, y, X_test=None, func='get_xgb_pred', params=None, res_transform=None, n_splits=3):\n",
    "    if X_test is None:\n",
    "        kf = KFold(n_splits=n_splits)\n",
    "        y_pred = np.zeros_like(y.values)\n",
    "        for train_inds, test_inds in kf.split(X):\n",
    "            y_pred[test_inds] = globals()[func](X.iloc[train_inds], X.iloc[test_inds],\n",
    "                                                y.iloc[train_inds], res_transform=res_transform, params=params)\n",
    "        return y_pred\n",
    "    else:\n",
    "        return globals()[func](X, X_test, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_stack = pd.DataFrame(index=train.index)\n",
    "test_stack = pd.DataFrame(index=test.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15min 28s, sys: 7.19 s, total: 15min 35s\n",
      "Wall time: 7min 13s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_stack['y_xgb'] = predict_outoffolds(train, np.log(y_train + SHIFT), func='get_xgb_pred')\n",
    "test_stack['y_xgb'] = predict_outoffolds(train, np.log(y_train + SHIFT), X_test=test, func='get_xgb_pred')\n",
    "train_stack['y_lightgbm'] = predict_outoffolds(train, np.log(y_train + SHIFT), func='get_lightgbm_pred')\n",
    "test_stack['y_lightgbm'] = predict_outoffolds(train, np.log(y_train + SHIFT), X_test=test, func='get_lightgbm_pred')\n",
    "train_stack['y_xgb'] = np.exp(train_stack.y_xgb) - SHIFT\n",
    "test_stack['y_xgb'] = np.exp(test_stack.y_xgb) - SHIFT\n",
    "train_stack['y_lightgbm'] = np.exp(train_stack.y_lightgbm) - SHIFT\n",
    "test_stack['y_lightgbm'] = np.exp(test_stack.y_lightgbm) - SHIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1168.72252642 1149.25187965 1162.57690445 1150.66772131\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(train_stack.y_xgb.fillna(0), y_train),\n",
    "      mean_absolute_error(train_stack.y_lightgbm.fillna(0), y_train),\n",
    "      mean_absolute_error(test_stack.y_xgb.fillna(0), y_test),\n",
    "      mean_absolute_error(test_stack.y_lightgbm.fillna(0), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fit linear regression without bias on this two features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1166.50388063\n"
     ]
    }
   ],
   "source": [
    "lr = LinearRegression(fit_intercept=False)\n",
    "lr.fit(train_stack[['y_xgb', 'y_lightgbm']], y_train)\n",
    "y_pred = lr.predict(test_stack[['y_xgb', 'y_lightgbm']])\n",
    "print(mean_absolute_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.420327063731 0.693043542054\n"
     ]
    }
   ],
   "source": [
    "print(*lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fit nonlinear model on second level\n",
    "concatenate meta features with original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_train = pd.concat([train, train_stack], axis=1)\n",
    "_test = pd.concat([test, test_stack], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1145.20823757\n",
      "CPU times: user 25.7 s, sys: 1.18 s, total: 26.9 s\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from pylightgbm.models import GBMRegressor\n",
    "params = {'verbose': False, 'num_iterations': 100}\n",
    "clf = GBMRegressor(**params)\n",
    "clf.fit(_train, np.log(y_train + SHIFT))\n",
    "y_pred = np.exp(clf.predict(_test)) - SHIFT\n",
    "print(mean_absolute_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better than convex combination of predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Try to add yet another LightGBM\n",
    "parameters are intentionally set to non-optimal values to get a weaker model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164.09046584\n",
      "CPU times: user 25.2 s, sys: 1.18 s, total: 26.3 s\n",
      "Wall time: 54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {\n",
    "    'num_iterations': 100,\n",
    "    'num_leaves': 127,\n",
    "    'feature_fraction': .5,\n",
    "    'bagging_fraction': .5,\n",
    "    'learning_rate': .06,\n",
    "    'min_data_in_leaf': 10,\n",
    "    'verbose': False\n",
    "}\n",
    "from pylightgbm.models import GBMRegressor\n",
    "clf = GBMRegressor(**params)\n",
    "clf.fit(train, np.log(y_train + SHIFT))\n",
    "y_pred_3 = np.exp(clf.predict(test)) - SHIFT\n",
    "print(mean_absolute_error(y_pred_3, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148.42467693\n"
     ]
    }
   ],
   "source": [
    "y_pred = (y_pred_1 + y_pred_2 + y_pred_3)/3\n",
    "print(mean_absolute_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weak model worsened the result of averaging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Convex combination of predictions tuned by Sequential Least Squares Programming solver\n",
    "https://github.com/jdwittenauer/kaggle/blob/master/OttoGroup/find_ensemble_weights.py\n",
    "\n",
    "http://stackoverflow.com/questions/35631192/element-wise-constraints-in-scipy-optimize-minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "def log_loss_func(weights):\n",
    "    \"\"\"\n",
    "    scipy minimize will pass the weights as a numpy array\n",
    "    \"\"\"\n",
    "    final_prediction = 0\n",
    "    for weight, prediction in zip(weights, preds):\n",
    "            final_prediction += weight*prediction\n",
    "    return mean_absolute_error(y_test, final_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "preds = [y_pred_1, y_pred_2, y_pred_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "init_weights = np.ones(len(preds)) / len(preds)\n",
    "# adding constraints\n",
    "cons = ({'type': 'eq', 'fun': lambda w: 1-sum(w)})  # weights are sum to 1\n",
    "bounds = [(0, 1)] * len(preds)  # and bounded between 0 and 1\n",
    "w = minimize(log_loss_func, init_weights, method='SLSQP', bounds=bounds, constraints=cons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.30029172,  0.46381807,  0.23589021])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1147.4365723451813"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_absolute_error(np.dot(w.x.reshape(1, -1), np.vstack([y_pred_1, y_pred_2, y_pred_3])).ravel(), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better than simple mean, but it is not the general case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Get out-of-fold prediction for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 27s, sys: 4.3 s, total: 1min 31s\n",
      "Wall time: 3min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_stack['y_lightgbm2'] = predict_outoffolds(train, np.log(y_train + SHIFT), func='get_lightgbm_pred', params=params)\n",
    "test_stack['y_lightgbm2'] = predict_outoffolds(train, np.log(y_train + SHIFT), X_test=test, func='get_lightgbm_pred',\n",
    "                                               params=params)\n",
    "train_stack['y_lightgbm2'] = np.exp(train_stack.y_lightgbm2) - SHIFT\n",
    "test_stack['y_lightgbm2'] = np.exp(test_stack.y_lightgbm2) - SHIFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1158.32348978 1150.66772131\n"
     ]
    }
   ],
   "source": [
    "print(mean_absolute_error(train_stack.y_lightgbm2.fillna(0), y_train),\n",
    "      mean_absolute_error(test_stack.y_lightgbm2.fillna(0), y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Fit nonlinear model on second level\n",
    "concatenate meta features with original features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_train = pd.concat([train, train_stack], axis=1)\n",
    "_test = pd.concat([test, test_stack], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1144.95280688\n",
      "CPU times: user 31.5 s, sys: 1.46 s, total: 33 s\n",
      "Wall time: 1min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "params = {'verbose': False, 'num_iterations': 100, 'num_leaves': 63}\n",
    "clf = GBMRegressor(**params)\n",
    "clf.fit(_train, np.log(y_train + SHIFT), test_data=[(_test, np.log(y_test + SHIFT))])\n",
    "y_pred = np.exp(clf.predict(_test)) - SHIFT\n",
    "print(mean_absolute_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Some links\n",
    "http://mlwave.com/kaggle-ensembling-guide/\n",
    "\n",
    "https://www.kaggle.com/c/allstate-claims-severity/forums/t/25743/stacking-understanding-python-package-for-stacking\n",
    "\n",
    "https://www.kaggle.com/c/allstate-claims-severity/forums/t/25240/weights-in-an-ensemble"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
